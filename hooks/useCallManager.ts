import { useState, useRef, useEffect, useCallback } from "react";
import { useSelector } from "react-redux";
import { RootState } from "@/libs/redux/store";
import { useSocket } from "@/context/socketContext";
import toast from "react-hot-toast";

type CallState =
  | "idle"
  | "calling"
  | "ringing"
  | "connecting"
  | "connected"
  | "ended"
  | "failed";
type ConnectionState =
  | "new"
  | "connecting"
  | "connected"
  | "disconnected"
  | "failed"
  | "closed";

  const isLowEndDevice = () => {
  const memory = (navigator as any).deviceMemory; // GB
  const hardwareConcurrency = navigator.hardwareConcurrency || 1;
  const isMobile = /Android|iPhone|iPad/i.test(navigator.userAgent);
  
  console.log("üîç Device Detection:", {
    memory: memory || "unknown",
    cores: hardwareConcurrency,
    isMobile,
    userAgent: navigator.userAgent,
  });
  
  // Consider low-end if:
  // - Less than 2GB RAM OR
  // - Less than 4 CPU cores OR
  // - Specific low-end device detected
  const isLowRam = memory && memory < 2;
  const isLowCpu = hardwareConcurrency < 4;
  const isItelOrLowEnd = /Itel|Symphony|Infinix|Tecno/i.test(navigator.userAgent);
  
  const isLowEnd = isMobile && (isLowRam || isLowCpu || isItelOrLowEnd);
  
  console.log(`üì± Device classified as: ${isLowEnd ? "LOW-END" : "HIGH-END"}`);
  
  return isLowEnd;
};

// ===== OPTIMIZED VIDEO CONSTRAINTS =====
const getOptimizedVideoConstraints = (isVideo: boolean) => {
  if (!isVideo) return false;
  
  const lowEnd = isLowEndDevice();
  
  if (lowEnd) {
    console.log("üì± Using LOW-END device settings (optimized for 1GB RAM)");
    return {
      width: { min: 320, ideal: 480, max: 640 },
      height: { min: 240, ideal: 360, max: 480 },
      frameRate: { min: 10, ideal: 15, max: 20 },
      facingMode: "user",
      aspectRatio: 4/3,
    };
  } else {
    console.log("üíª Using HIGH-END device settings");
    return {
      width: { min: 640, ideal: 1280, max: 1920 },
      height: { min: 480, ideal: 720, max: 1080 },
      frameRate: { ideal: 30, max: 30 },
      facingMode: "user",
    };
  }
};

// ===== STORAGE CHECK =====
const checkStorageSpace = async () => {
  if ('storage' in navigator && 'estimate' in navigator.storage) {
    try {
      const estimate = await navigator.storage.estimate();
      const usedMB = (estimate.usage || 0) / (1024 * 1024);
      const totalMB = (estimate.quota || 0) / (1024 * 1024);
      const freeMB = totalMB - usedMB;
      
      console.log("üíæ Storage Info:", {
        used: `${usedMB.toFixed(0)}MB`,
        total: `${totalMB.toFixed(0)}MB`,
        free: `${freeMB.toFixed(0)}MB`,
      });
      
      if (freeMB < 200) {
        console.warn("‚ö†Ô∏è WARNING: Low storage space! Video calls may fail.");
        return false;
      }
      
      return true;
    } catch (error) {
      console.error("Could not estimate storage:", error);
      return true; // Assume OK if can't check
    }
  }
  
  return true; // Assume OK if API not supported
};

// ===== MEMORY PRESSURE DETECTION =====
const checkMemoryPressure = () => {
  const memory = (performance as any).memory;
  
  if (memory) {
    const usedMB = memory.usedJSHeapSize / (1024 * 1024);
    const limitMB = memory.jsHeapSizeLimit / (1024 * 1024);
    const percentUsed = (usedMB / limitMB) * 100;
    
    console.log("üß† Memory Usage:", {
      used: `${usedMB.toFixed(0)}MB`,
      limit: `${limitMB.toFixed(0)}MB`,
      percent: `${percentUsed.toFixed(1)}%`,
    });
    
    if (percentUsed > 80) {
      console.warn("‚ö†Ô∏è WARNING: High memory pressure! Recommend closing other apps.");
      return false;
    }
  }
  
  return true;
};

// ===== OPTIMIZED AUDIO CONSTRAINTS (Lower bandwidth) =====
const getOptimizedAudioConstraints = () => {
  const lowEnd = isLowEndDevice();
  
  if (lowEnd) {
    console.log("üîä Using LOW-END audio settings");
    return {
      echoCancellation: true,
      noiseSuppression: true,
      autoGainControl: true,
      sampleRate: 16000, // Lower sample rate (was 48000)
      channelCount: 1,    // Mono (was default 2/stereo)
    };
  } else {
    return {
      echoCancellation: true,
      noiseSuppression: true,
      autoGainControl: true,
      sampleRate: 48000,
    };
  }
};

export const useCallManagement = (currentChat: any) => {
  const [callState, setCallState] = useState<CallState>("idle");
  const [connectionState, setConnectionState] =
    useState<ConnectionState>("new");
  const [isVideoCall, setIsVideoCall] = useState(false);
  const [callError, setCallError] = useState<string | null>(null);
  const [callDuration, setCallDuration] = useState(0);
  const [isAudioMuted, setIsAudioMuted] = useState(false);
  const [isVideoMuted, setIsVideoMuted] = useState(false);
  const [isRemoteAudioMuted, setIsRemoteAudioMuted] = useState(false);
  const [isCallMinimized, setIsCallMinimized] = useState(false);
  const [localStream, setLocalStream] = useState<MediaStream | null>(null);
  const [remoteStream, setRemoteStream] = useState<MediaStream | null>(null);
  const [incomingCall, setIncomingCall] = useState<{
    from: string;
    isVideo: boolean;
    callId: string;
  } | null>(null);

  const localVideoRef = useRef<HTMLVideoElement>(null);
  const remoteVideoRef = useRef<HTMLVideoElement>(null);
  const remoteAudioRef = useRef<HTMLAudioElement | null>(null);
  const peerConnectionRef = useRef<RTCPeerConnection | null>(null);
  const callStartTimeRef = useRef<number | null>(null);
  const callTimerRef = useRef<NodeJS.Timeout | null>(null);
  const reconnectTimeoutRef = useRef<NodeJS.Timeout | null>(null);
  const currentCallIdRef = useRef<string | null>(null);
  const callTimeoutRef = useRef<NodeJS.Timeout | null>(null);
  const reconnectAttemptsRef = useRef(0);
  const maxReconnectAttempts = 5;
  const callStateRef = useRef<CallState>("idle");
  const iceCandidateQueue = useRef<RTCIceCandidateInit[]>([]);
  
  // üî¥ CRITICAL: Track streams separately
  const remoteAudioStreamRef = useRef<MediaStream | null>(null);
  const remoteVideoStreamRef = useRef<MediaStream | null>(null);

  const { socket, isConnected } = useSocket();
  const { user } = useSelector((state: RootState) => state.auth);

  const otherUserId = useRef<string | null>(null);

  useEffect(() => {
    if (currentChat?.type !== "group") {
      const other = currentChat?.participants?.find(
        (p: any) => p._id !== user?._id
      )?._id;
      otherUserId.current = other || null;
    } else {
      otherUserId.current = null;
    }
  }, [currentChat, user?._id]);

  useEffect(() => {
    callStateRef.current = callState;
  }, [callState]);

  // üî¥ CRITICAL FIX: Create audio element WITHOUT any video properties
  useEffect(() => {
    if (!remoteAudioRef.current) {
      const audio = document.createElement("audio");
      audio.autoplay = true;
      audio.muted = false; // MUST be false for audio
      audio.volume = 1.0;
      document.body.appendChild(audio);
      console.log("üîä Created dedicated audio element for remote audio ONLY");
    }

    return () => {
      if (remoteAudioRef.current) {
        remoteAudioRef.current.pause();
        remoteAudioRef.current.srcObject = null;
        remoteAudioRef.current.remove();
        remoteAudioRef.current = null;
      }
    };
  }, []);

  const configuration = {
    iceServers: [
      { urls: "stun:stun.l.google.com:19302" },
      { urls: "stun:stun1.l.google.com:19302" },
      {
        urls: [
          "turn:jb-turn1.xirsys.com:80?transport=udp",
          "turn:jb-turn1.xirsys.com:3478?transport=udp",
          "turn:jb-turn1.xirsys.com:80?transport=tcp",
          "turn:jb-turn1.xirsys.com:3478?transport=tcp",
          "turns:jb-turn1.xirsys.com:443?transport=tcp",
          "turns:jb-turn1.xirsys.com:5349?transport=tcp",
        ],
        username:
          "-vkE_HbUPxWY81OqkAwG7uEpErSNCRqPTX7nP6JyC8jwqzmrmSjtljr7ugCfPoayAAAAAGiBFnxxYXl5dW0=",
        credential: "515cb5a6-67e7-11f0-b95a-0242ac120004",
      },
    ],
    iceCandidatePoolSize: 10,
    bundlePolicy: "max-bundle" as RTCBundlePolicy,
    rtcpMuxPolicy: "require" as RTCRtcpMuxPolicy,
    iceTransportPolicy: 'all' as RTCIceTransportPolicy,
  };

  const formatDuration = (seconds: number) => {
    const mins = Math.floor(seconds / 60);
    const secs = seconds % 60;
    return `${mins.toString().padStart(2, "0")}:${secs
      .toString()
      .padStart(2, "0")}`;
  };

  const startCallTimer = useCallback(() => {
    callStartTimeRef.current = Date.now();
    callTimerRef.current = setInterval(() => {
      if (callStartTimeRef.current) {
        setCallDuration(
          Math.floor((Date.now() - callStartTimeRef.current) / 1000)
        );
      }
    }, 1000);
  }, []);

  const stopCallTimer = useCallback(() => {
    if (callTimerRef.current) {
      clearInterval(callTimerRef.current);
      callTimerRef.current = null;
    }
    if (reconnectTimeoutRef.current) {
      clearTimeout(reconnectTimeoutRef.current);
      reconnectTimeoutRef.current = null;
    }
    callStartTimeRef.current = null;
    setCallDuration(0);
  }, []);

  const cleanup = useCallback(() => {
    console.log("üßπ Cleaning up call resources...");

    if (localStream) {
      localStream.getTracks().forEach((track) => {
        track.stop();
        console.log("üõë Stopped local track:", track.kind);
      });
      setLocalStream(null);
    }

    if (remoteStream) {
      remoteStream.getTracks().forEach((track) => {
        track.stop();
        console.log("üõë Stopped remote track:", track.kind);
      });
      setRemoteStream(null);
    }

    if (remoteAudioStreamRef.current) {
      remoteAudioStreamRef.current.getTracks().forEach((track) => track.stop());
      remoteAudioStreamRef.current = null;
    }

    if (remoteVideoStreamRef.current) {
      remoteVideoStreamRef.current.getTracks().forEach((track) => track.stop());
      remoteVideoStreamRef.current = null;
    }

    if (peerConnectionRef.current) {
      peerConnectionRef.current.close();
      peerConnectionRef.current = null;
    }

    if (localVideoRef.current) {
      localVideoRef.current.srcObject = null;
      localVideoRef.current.pause();
    }
    if (remoteVideoRef.current) {
      remoteVideoRef.current.srcObject = null;
      remoteVideoRef.current.pause();
    }
    if (remoteAudioRef.current) {
      remoteAudioRef.current.srcObject = null;
      remoteAudioRef.current.pause();
    }

    if (callTimeoutRef.current) {
      clearTimeout(callTimeoutRef.current);
      callTimeoutRef.current = null;
    }

    stopCallTimer();
    currentCallIdRef.current = null;
    reconnectAttemptsRef.current = 0;
    iceCandidateQueue.current = [];

    setCallState("idle");
    setConnectionState("new");
    setCallError(null);
    setIsAudioMuted(false);
    setIsVideoMuted(false);
    setIsRemoteAudioMuted(false);
    setIsCallMinimized(false);
    setIncomingCall(null);
  }, [localStream, remoteStream, stopCallTimer]);

const initializePeerConnection = useCallback(() => {
  if (peerConnectionRef.current) {
    console.log("üîå Closing existing peer connection");
    peerConnectionRef.current.close();
  }

  console.log("üîå Creating new RTCPeerConnection");
  const pc = new RTCPeerConnection(configuration);
  peerConnectionRef.current = pc;

  // ===== ICE CANDIDATE HANDLER =====
  pc.onicecandidate = (event) => {
    if (event.candidate) {
      console.log("üì§ ICE candidate:", event.candidate.type, event.candidate.protocol);
      if (socket && otherUserId.current && currentCallIdRef.current) {
        socket.emit("ice-candidate", {
          candidate: event.candidate,
          to: otherUserId.current,
          callId: currentCallIdRef.current,
        });
      }
    } else {
      console.log("‚úÖ ICE gathering complete");
    }
  };

  // ===== üî¥ CRITICAL: TRACK HANDLER (FIXED) =====
  pc.ontrack = (event) => {
    console.log("üì• ===== ONTRACK FIRED =====");
    console.log("üì• Track kind:", event.track.kind);
    console.log("üì• Track ID:", event.track.id);
    console.log("üì• Track label:", event.track.label);
    console.log("üì• Track enabled:", event.track.enabled);
    console.log("üì• Track muted:", event.track.muted);
    console.log("üì• Track readyState:", event.track.readyState);
    console.log("üì• Event streams count:", event.streams.length);

    // Store complete remote stream
    if (event.streams && event.streams[0]) {
      const stream = event.streams[0];
      console.log("üì• Stream ID:", stream.id);
      console.log("üì• Stream tracks:", {
        audio: stream.getAudioTracks().length,
        video: stream.getVideoTracks().length,
      });
      setRemoteStream(stream);
    }

    // üî¥ CRITICAL: Handle AUDIO track
    if (event.track.kind === "audio") {
      console.log("üîä ===== AUDIO TRACK RECEIVED =====");

      // Create audio-only stream
      const audioOnlyStream = new MediaStream([event.track]);
      remoteAudioStreamRef.current = audioOnlyStream;

      console.log("üîä Audio stream created:", {
        streamId: audioOnlyStream.id,
        audioTracks: audioOnlyStream.getAudioTracks().length,
        videoTracks: audioOnlyStream.getVideoTracks().length,
      });

      // üî¥ CRITICAL: Ensure audio element exists
      if (!remoteAudioRef.current) {
        console.error("‚ùå Audio element missing! Creating now...");
        const audio = document.createElement("audio");
        audio.autoplay = true;
        audio.muted = false;
        audio.volume = 1.0;
        document.body.appendChild(audio);
        remoteAudioRef.current = audio;
        console.log("‚úÖ Audio element created");
      }

      console.log("üîä Setting audio stream to audio element");
      remoteAudioRef.current.srcObject = audioOnlyStream;
      remoteAudioRef.current.muted = false; // MUST be false
      remoteAudioRef.current.volume = 1.0;

      console.log("üîä Audio element configured:", {
        srcObject: !!remoteAudioRef.current.srcObject,
        muted: remoteAudioRef.current.muted,
        volume: remoteAudioRef.current.volume,
      });

      // üî¥ CRITICAL: Force play
      console.log("üîä Attempting to play audio...");
      const playPromise = remoteAudioRef.current.play();

      if (playPromise !== undefined) {
        playPromise
          .then(() => {
            console.log("‚úÖ‚úÖ‚úÖ AUDIO IS PLAYING!");
            console.log("üîä Audio state:", {
              paused: remoteAudioRef.current?.paused,
              currentTime: remoteAudioRef.current?.currentTime,
              volume: remoteAudioRef.current?.volume,
            });

            // Monitor audio continuously
            const monitorAudio = setInterval(() => {
              if (remoteAudioRef.current) {
                // Auto-resume if paused
                if (remoteAudioRef.current.paused) {
                  console.warn("‚ö†Ô∏è Audio paused! Resuming...");
                  remoteAudioRef.current.play();
                }
              } else {
                clearInterval(monitorAudio);
              }
            }, 2000);
          })
          .catch((err) => {
            console.error("‚ùå Audio play failed:", err.name, err.message);

            if (err.name === "NotAllowedError") {
              console.log("üîß User interaction needed for audio");
              
              // Setup interaction listener
              const enableAudio = () => {
                console.log("üëÜ Playing audio after interaction");
                remoteAudioRef.current?.play()
                  .then(() => console.log("‚úÖ Audio now playing"))
                  .catch(e => console.error("‚ùå Still failed:", e));
                document.removeEventListener("click", enableAudio);
                document.removeEventListener("touchstart", enableAudio);
              };

              document.addEventListener("click", enableAudio, { once: true });
              document.addEventListener("touchstart", enableAudio, { once: true });
              
              toast("Tap screen to enable audio", { icon: "üîä" });
            }
          });
      }

      // Monitor track state
      event.track.onended = () => console.warn("‚ö†Ô∏è Audio track ended");
      event.track.onmute = () => console.warn("‚ö†Ô∏è Audio track muted");
      event.track.onunmute = () => console.log("‚úÖ Audio track unmuted");
    }

    // üî¥ CRITICAL: Handle VIDEO track
    if (event.track.kind === "video") {
      console.log("üìπ ===== VIDEO TRACK RECEIVED =====");

      // Create video-only stream
      const videoOnlyStream = new MediaStream([event.track]);
      remoteVideoStreamRef.current = videoOnlyStream;

      console.log("üìπ Video stream created:", {
        streamId: videoOnlyStream.id,
        audioTracks: videoOnlyStream.getAudioTracks().length,
        videoTracks: videoOnlyStream.getVideoTracks().length,
      });

      if (!remoteVideoRef.current) {
        console.error("‚ùå Video element missing!");
        return;
      }

      console.log("üìπ Setting video stream to video element");

      // Clear existing
      remoteVideoRef.current.srcObject = null;
      remoteVideoRef.current.pause();

      // Set new stream with delay
      setTimeout(() => {
        if (remoteVideoRef.current && videoOnlyStream) {
          remoteVideoRef.current.srcObject = videoOnlyStream;

          // üî¥ CRITICAL: Video MUST be muted (audio from separate element)
          remoteVideoRef.current.muted = true;
          remoteVideoRef.current.volume = 0;
          remoteVideoRef.current.autoplay = true;
          remoteVideoRef.current.playsInline = true;

          // Mobile attributes
          remoteVideoRef.current.setAttribute("playsinline", "true");
          remoteVideoRef.current.setAttribute("webkit-playsinline", "true");
          remoteVideoRef.current.setAttribute("x5-playsinline", "true");
          remoteVideoRef.current.setAttribute("x5-video-player-type", "h5");
          remoteVideoRef.current.setAttribute("x5-video-player-fullscreen", "false");

          console.log("üìπ Video element configured");

          // Force play
          const playPromise = remoteVideoRef.current.play();

          if (playPromise !== undefined) {
            playPromise
              .then(() => {
                console.log("‚úÖ‚úÖ‚úÖ VIDEO IS PLAYING!");
                console.log("üìπ Video state:", {
                  paused: remoteVideoRef.current?.paused,
                  videoWidth: remoteVideoRef.current?.videoWidth,
                  videoHeight: remoteVideoRef.current?.videoHeight,
                });
              })
              .catch((err) => {
                console.error("‚ùå Video play failed:", err.name);

                // Setup interaction listener
                const enableVideo = () => {
                  console.log("üëÜ Playing video after interaction");
                  if (remoteVideoRef.current) {
                    remoteVideoRef.current.muted = true;
                    remoteVideoRef.current.play()
                      .then(() => console.log("‚úÖ Video now playing"))
                      .catch(e => console.error("‚ùå Still failed:", e));
                  }
                  document.removeEventListener("click", enableVideo);
                  document.removeEventListener("touchstart", enableVideo);
                };

                document.addEventListener("click", enableVideo, { once: true });
                document.addEventListener("touchstart", enableVideo, { once: true });
              });
          }
        }
      }, 200);

      // Monitor track state
      event.track.onended = () => console.warn("‚ö†Ô∏è Video track ended");
      event.track.onmute = () => console.warn("‚ö†Ô∏è Video track muted");
      event.track.onunmute = () => console.log("‚úÖ Video track unmuted");
    }
  };

  // ===== CONNECTION STATE HANDLERS =====
  pc.oniceconnectionstatechange = () => {
    console.log("üßä ICE connection state:", pc.iceConnectionState);
  };

  pc.onicegatheringstatechange = () => {
    console.log("üßä ICE gathering state:", pc.iceGatheringState);
  };

  pc.onsignalingstatechange = () => {
    console.log("üì° Signaling state:", pc.signalingState);
  };

  pc.onconnectionstatechange = () => {
    const state = pc.connectionState;
    console.log("üîó Connection state:", state);
    setConnectionState(state as ConnectionState);

    if (state === "connected") {
      console.log("‚úÖ ===== PEER CONNECTION CONNECTED =====");

      // Log all receivers
      const receivers = pc.getReceivers();
      console.log("üìä Active receivers:", receivers.length);
      receivers.forEach((receiver) => {
        if (receiver.track) {
          console.log(`üìä ${receiver.track.kind}:`, {
            enabled: receiver.track.enabled,
            readyState: receiver.track.readyState,
            muted: receiver.track.muted,
          });
        }
      });

      setCallState("connected");
      setCallError(null);
      reconnectAttemptsRef.current = 0;
      
      if (callStartTimeRef.current === null) {
        startCallTimer();
      }
      
      toast.success("Call connected");

      // üî¥ CRITICAL: Debug audio after connection
      setTimeout(() => {
        console.log("üîç ===== POST-CONNECTION AUDIO CHECK =====");
        if (remoteAudioRef.current) {
          console.log("üîä Audio element:", {
            exists: true,
            srcObject: !!remoteAudioRef.current.srcObject,
            muted: remoteAudioRef.current.muted,
            volume: remoteAudioRef.current.volume,
            paused: remoteAudioRef.current.paused,
          });

          const stream = remoteAudioRef.current.srcObject as MediaStream;
          if (stream) {
            console.log("üîä Audio stream:", {
              active: stream.active,
              audioTracks: stream.getAudioTracks().length,
            });

            stream.getAudioTracks().forEach((track, idx) => {
              console.log(`üîä Audio track ${idx}:`, {
                enabled: track.enabled,
                muted: track.muted,
                readyState: track.readyState,
              });
            });
          } else {
            console.error("‚ùå No audio stream!");
          }
        } else {
          console.error("‚ùå No audio element!");
        }
        console.log("üîç ===== END AUDIO CHECK =====");
      }, 2000);

    } else if (state === "disconnected") {
      setCallError("Connection interrupted. Reconnecting...");
      if (reconnectAttemptsRef.current < maxReconnectAttempts) {
        reconnectAttemptsRef.current++;
        reconnectTimeoutRef.current = setTimeout(() => {
          if (pc.connectionState === "disconnected") {
            console.log("üîÑ Attempting ICE restart...");
            pc.restartIce();
          }
        }, 2000);
      }
    } else if (state === "failed") {
      console.error("‚ùå Connection failed");
      setCallError("Connection failed");
      endCall();
    }
  };

  return pc;
}, [socket, startCallTimer]);


  const startCall = useCallback(
  async (isVideo: boolean = false) => {
    console.log("üìû ===== STARTING CALL =====");
    console.log("üìû Video:", isVideo, "To:", otherUserId.current);

    if (!currentChat || !socket || !otherUserId.current) {
      toast.error("Cannot start call");
      return;
    }

    if (!isConnected) {
      toast.error("Not connected to server");
      return;
    }

    try {
      // üî¥ CRITICAL: Check device capabilities
      const hasStorage = await checkStorageSpace();
      const hasMemory = checkMemoryPressure();
      
      if (!hasStorage) {
        toast.error("Low storage space! Free up at least 200MB and try again.");
        return;
      }
      
      if (!hasMemory) {
        toast("High memory usage. Close other apps for better performance.", {
          icon: "‚ö†Ô∏è",
          duration: 5000,
        });
      }

      setCallError(null);
      setCallState("calling");
      setIsVideoCall(isVideo);

      const callId = `${user?._id}-${otherUserId.current}-${Date.now()}`;
      currentCallIdRef.current = callId;

      console.log("üé§ Requesting user media with optimized settings...");

      const stream = await navigator.mediaDevices.getUserMedia({
        audio: getOptimizedAudioConstraints(),
        video: getOptimizedVideoConstraints(isVideo),
      });

      console.log("‚úÖ Got local media stream");
      console.log(
        "üìä Local stream tracks:",
        stream.getTracks().map((t) => ({
          kind: t.kind,
          id: t.id,
          label: t.label,
          enabled: t.enabled,
          readyState: t.readyState,
          settings: t.getSettings ? t.getSettings() : "N/A",
        }))
      );

      // Log actual video settings
      const videoTrack = stream.getVideoTracks()[0];
      if (videoTrack && videoTrack.getSettings) {
        const settings = videoTrack.getSettings();
        console.log("üìπ Actual video settings:", {
          width: settings.width,
          height: settings.height,
          frameRate: settings.frameRate,
          aspectRatio: settings.aspectRatio,
        });
      }

      setLocalStream(stream);
      
      // Local video MUST be muted
      if (localVideoRef.current) {
        localVideoRef.current.srcObject = stream;
        localVideoRef.current.muted = true;
        localVideoRef.current.volume = 0;
        localVideoRef.current.playsInline = true;
        localVideoRef.current.autoplay = true;

        localVideoRef.current.play().catch(err => {
          console.error("Local video play failed:", err);
        });
        
        console.log("‚úÖ Local video configured (muted)");
      }

      console.log("üîå Initializing peer connection...");
      const pc = initializePeerConnection();

      stream.getTracks().forEach((track) => {
        console.log("‚ûï Adding track to PC:", track.kind, track.id);
        const sender = pc.addTrack(track, stream);
        console.log("‚úÖ Track added, sender:", sender);
      });

      console.log(
        "üìä Peer connection senders:",
        pc.getSenders().map((s) => ({
          kind: s.track?.kind,
          id: s.track?.id,
          enabled: s.track?.enabled,
        }))
      );

      console.log("üì§ Sending call_request");
      socket.emit("call_request", {
        to: otherUserId.current,
        isVideo,
        callId,
      });

      await new Promise((resolve) => setTimeout(resolve, 300));

      console.log("üìù Creating offer...");
      const offer = await pc.createOffer({
        offerToReceiveAudio: true,
        offerToReceiveVideo: isVideo,
      });

      console.log("üìù Setting local description...");
      await pc.setLocalDescription(offer);
      console.log("‚úÖ Local description set:", pc.localDescription?.type);

      console.log("üì§ Sending offer to", otherUserId.current);
      socket.emit("offer", {
        sdp: offer,
        to: otherUserId.current,
        isVideo,
        callId,
      });

      callTimeoutRef.current = setTimeout(() => {
        if (
          callStateRef.current === "calling" ||
          callStateRef.current === "ringing"
        ) {
          setCallError("No answer");
          toast.error("No answer");
          socket.emit("call_timeout", { to: otherUserId.current, callId });
          setTimeout(() => cleanup(), 2000);
        }
      }, 45000);
    } catch (error: any) {
      console.error("‚ùå Error starting call:", error);
      let message = "Failed to start call";

      if (error.name === "NotAllowedError") {
        message = "Camera/microphone permission denied";
      } else if (error.name === "NotFoundError") {
        message = "No camera or microphone found";
      } else if (error.name === "NotReadableError") {
        message = "Camera/microphone already in use";
      } else if (error.name === "OverconstrainedError") {
        message = "Camera doesn't support requested resolution";
        console.error("OverconstrainedError details:", error.constraint);
      }

      setCallError(message);
      setCallState("failed");
      toast.error(message);
      setTimeout(() => cleanup(), 2000);
    }
  },
  [
    currentChat,
    socket,
    isConnected,
    initializePeerConnection,
    cleanup,
    user?._id,
  ]
);

const acceptCall = useCallback(async () => {
  console.log("‚úÖ ===== ACCEPTING CALL =====");

  if (!incomingCall || !socket) {
    console.error("‚ùå Cannot accept - invalid state");
    return;
  }

  try {
    // üî¥ CRITICAL: Check device capabilities
    const hasStorage = await checkStorageSpace();
    const hasMemory = checkMemoryPressure();
    
    if (!hasStorage) {
      toast.error("Low storage space! Free up at least 200MB.");
      declineCall();
      return;
    }
    
    if (!hasMemory) {
      toast("High memory usage. Close other apps for better performance.", {
        icon: "‚ö†Ô∏è",
        duration: 5000,
      });
    }

    setCallState("connecting");
    const callData = { ...incomingCall };
    setIncomingCall(null);
    setIsVideoCall(callData.isVideo);

    if (callTimeoutRef.current) {
      clearTimeout(callTimeoutRef.current);
      callTimeoutRef.current = null;
    }

    console.log("üé§ Requesting user media with optimized settings...");

    // üî¥ USE OPTIMIZED CONSTRAINTS
    const stream = await navigator.mediaDevices.getUserMedia({
      audio: getOptimizedAudioConstraints(),
      video: getOptimizedVideoConstraints(callData.isVideo),
    });

    console.log("‚úÖ Got local media stream");
    console.log(
      "üìä Local stream tracks:",
      stream.getTracks().map((t) => ({
        kind: t.kind,
        id: t.id,
        label: t.label,
        enabled: t.enabled,
        readyState: t.readyState,
        settings: t.getSettings ? t.getSettings() : "N/A",
      }))
    );

    // Log actual video settings
    const videoTrack = stream.getVideoTracks()[0];
    if (videoTrack && videoTrack.getSettings) {
      const settings = videoTrack.getSettings();
      console.log("üìπ Actual video settings:", {
        width: settings.width,
        height: settings.height,
        frameRate: settings.frameRate,
        aspectRatio: settings.aspectRatio,
      });
    }

    setLocalStream(stream);
    
    // Local video MUST be muted
    if (localVideoRef.current) {
      localVideoRef.current.srcObject = stream;
      localVideoRef.current.muted = true;
      localVideoRef.current.volume = 0;
      localVideoRef.current.playsInline = true;
      localVideoRef.current.autoplay = true;
      
      localVideoRef.current.play().catch(err => {
        console.error("Local video play failed:", err);
      });
      
      console.log("‚úÖ Local video configured (muted)");
    }

    let pc = peerConnectionRef.current;
    if (!pc) {
      console.log("‚ö†Ô∏è No PC, creating new one");
      pc = initializePeerConnection();
    }

    stream.getTracks().forEach((track) => {
      console.log("‚ûï Adding track to PC:", track.kind, track.id);
      const sender = pc.addTrack(track, stream);
      console.log("‚úÖ Track added, sender:", sender);
    });

    console.log(
      "üìä Peer connection senders:",
      pc.getSenders().map((s) => ({
        kind: s.track?.kind,
        id: s.track?.id,
        enabled: s.track?.enabled,
      }))
    );

    if (!pc.remoteDescription) {
      console.log("‚è≥ Waiting for offer...");
      await new Promise((resolve) => setTimeout(resolve, 1000));
    }

    if (!pc.remoteDescription) {
      console.error("‚ùå Still no remote description!");
      socket.emit("call_accept", {
        to: callData.from,
        callId: callData.callId,
      });
      return;
    }

    console.log("üìù Creating answer...");
    const answer = await pc.createAnswer();

    console.log("üìù Setting local description...");
    await pc.setLocalDescription(answer);
    console.log("‚úÖ Local description set:", pc.localDescription?.type);

    while (iceCandidateQueue.current.length > 0) {
      const candidate = iceCandidateQueue.current.shift();
      if (candidate) {
        console.log("üßä Adding queued ICE candidate");
        await pc.addIceCandidate(new RTCIceCandidate(candidate));
      }
    }

    console.log("üì§ Sending answer");
    socket.emit("answer", {
      sdp: answer,
      to: callData.from,
      callId: callData.callId,
    });

    console.log("üì§ Sending call_accept");
    socket.emit("call_accept", {
      to: callData.from,
      callId: callData.callId,
    });
  } catch (error: any) {
    console.error("‚ùå Error accepting call:", error);
    let message = "Failed to accept call";
    
    if (error.name === "NotAllowedError") {
      message = "Camera/microphone permission denied";
    } else if (error.name === "NotFoundError") {
      message = "No camera or microphone found";
    } else if (error.name === "NotReadableError") {
      message = "Camera/microphone already in use";
    } else if (error.name === "OverconstrainedError") {
      message = "Camera doesn't support requested resolution";
      console.error("OverconstrainedError details:", error.constraint);
    }
    
    setCallError(message);
    toast.error(message);
    cleanup();
  }
}, [incomingCall, socket, initializePeerConnection, cleanup]);

  const declineCall = useCallback(() => {
    if (incomingCall && socket) {
      socket.emit("call_decline", {
        to: incomingCall.from,
        callId: incomingCall.callId,
      });
      setIncomingCall(null);
      setCallState("idle");
      if (callTimeoutRef.current) {
        clearTimeout(callTimeoutRef.current);
        callTimeoutRef.current = null;
      }
    }
  }, [incomingCall, socket]);

  const endCall = useCallback(() => {
    console.log("üîö Ending call...");

    if (
      socket &&
      otherUserId.current &&
      callStateRef.current !== "idle" &&
      currentCallIdRef.current
    ) {
      socket.emit("call_end", {
        to: otherUserId.current,
        callId: currentCallIdRef.current,
      });
    }

    cleanup();
    toast.success("Call ended");
  }, [socket, cleanup]);

  const toggleAudioMute = useCallback(() => {
    if (localStream) {
      const audioTrack = localStream.getAudioTracks()[0];
      if (audioTrack) {
        audioTrack.enabled = !audioTrack.enabled;
        setIsAudioMuted(!audioTrack.enabled);
        console.log("üé§ Microphone:", audioTrack.enabled ? "ON" : "OFF");
      }
    }
  }, [localStream]);

  const toggleVideoMute = useCallback(() => {
    if (localStream) {
      const videoTrack = localStream.getVideoTracks()[0];
      if (videoTrack) {
        videoTrack.enabled = !videoTrack.enabled;
        setIsVideoMuted(!videoTrack.enabled);
        console.log("üìπ Camera:", videoTrack.enabled ? "ON" : "OFF");
      }
    }
  }, [localStream]);

  const toggleRemoteAudio = useCallback(() => {
    if (remoteAudioRef.current) {
      remoteAudioRef.current.muted = !remoteAudioRef.current.muted;
      setIsRemoteAudioMuted(remoteAudioRef.current.muted);
      console.log("üîä Speaker:", remoteAudioRef.current.muted ? "OFF" : "ON");
    }
  }, []);

  const switchCallType = useCallback(async () => {
    if (!peerConnectionRef.current || callState !== "connected") return;

    try {
      const newIsVideo = !isVideoCall;
      const stream = await navigator.mediaDevices.getUserMedia({
        video: newIsVideo,
        audio: true,
      });

      const sender = peerConnectionRef.current
        .getSenders()
        .find((s) => s.track && s.track.kind === "video");

      if (newIsVideo && !sender) {
        const videoTrack = stream.getVideoTracks()[0];
        if (videoTrack) {
          peerConnectionRef.current.addTrack(videoTrack, stream);
        }
      } else if (!newIsVideo && sender) {
        peerConnectionRef.current.removeTrack(sender);
      } else if (newIsVideo && sender) {
        const videoTrack = stream.getVideoTracks()[0];
        if (videoTrack) {
          await sender.replaceTrack(videoTrack);
        }
      }

      setLocalStream(stream);
      if (localVideoRef.current) {
        localVideoRef.current.srcObject = stream;
        localVideoRef.current.muted = true;
        localVideoRef.current.volume = 0;
      }
      setIsVideoCall(newIsVideo);
    } catch (error) {
      console.error("Failed to switch call type:", error);
      setCallError("Failed to switch call type");
    }
  }, [callState, isVideoCall]);

  // Socket event handlers
  useEffect(() => {
    if (!socket || !isConnected) return;

    console.log("üîå Setting up socket handlers...");

    const handleCallError = (data: { error: string; reason?: string }) => {
      console.log("‚ùå call_error:", data);
      setCallError(data.error);
      toast.error(data.error);
    };

    const handleCallWaiting = (data: { message: string; status: string }) => {
      console.log('‚è≥ call_waiting:', data);
      if (data.status === 'offline') {
        setCallError('Calling... (user offline)');
      } else if (data.status === 'online') {
        setCallError(null);
        setCallState('ringing');
      }
    };

    const handleOffer = async (data: {
      sdp: RTCSessionDescriptionInit;
      from: string;
      isVideo: boolean;
      callId: string;
    }) => {
      console.log("üì• ===== RECEIVED OFFER =====");
      console.log("üì• From:", data.from, "CallID:", data.callId);

      let pc = peerConnectionRef.current;
      if (!pc) {
        console.log("‚ö†Ô∏è No PC, creating one");
        pc = initializePeerConnection();
      }

      try {
        console.log("üìù Setting remote description (offer)...");
        await pc.setRemoteDescription(new RTCSessionDescription(data.sdp));
        console.log("‚úÖ Remote description set");
        console.log("üìä Remote description type:", pc.remoteDescription?.type);

        if (localStream) {
          console.log("üìù Creating answer (have local stream)...");

          const senders = pc.getSenders();
          localStream.getTracks().forEach((track) => {
            if (!senders.find((s) => s.track === track)) {
              console.log("‚ûï Adding track:", track.kind);
              pc.addTrack(track, localStream);
            }
          });

          const answer = await pc.createAnswer();
          await pc.setLocalDescription(answer);
          console.log("‚úÖ Answer created and set");

          socket.emit("answer", {
            sdp: answer,
            to: data.from,
            callId: data.callId,
          });
          console.log("üì§ Answer sent");
        }
      } catch (err) {
        console.error("‚ùå Error handling offer:", err);
      }
    };

    const handleAnswer = async (data: {
      sdp: RTCSessionDescriptionInit;
      from: string;
    }) => {
      console.log("üì• ===== RECEIVED ANSWER =====");
      console.log("üì• From:", data.from);

      const pc = peerConnectionRef.current;
      if (!pc) {
        console.error("‚ùå No PC for answer");
        return;
      }

      try {
        console.log("üìù Setting remote description (answer)...");
        await pc.setRemoteDescription(new RTCSessionDescription(data.sdp));
        console.log("‚úÖ Remote description set");
        console.log("üìä Remote description type:", pc.remoteDescription?.type);

        while (iceCandidateQueue.current.length > 0) {
          const candidate = iceCandidateQueue.current.shift();
          if (candidate) {
            console.log("üßä Adding queued ICE candidate");
            await pc.addIceCandidate(new RTCIceCandidate(candidate));
          }
        }

        setCallState("connected");
      } catch (err) {
        console.error("‚ùå Error setting answer:", err);
      }
    };

    const handleIceCandidate = async (data: {
      candidate: RTCIceCandidateInit;
      from: string;
    }) => {
      console.log("üì• ICE candidate from:", data.from);

      const pc = peerConnectionRef.current;
      if (!pc) {
        console.log("‚ö†Ô∏è No PC, queuing");
        iceCandidateQueue.current.push(data.candidate);
        return;
      }

      if (!pc.remoteDescription) {
        console.log("‚ö†Ô∏è No remote desc, queuing");
        iceCandidateQueue.current.push(data.candidate);
        return;
      }

      try {
        await pc.addIceCandidate(new RTCIceCandidate(data.candidate));
        console.log("‚úÖ ICE candidate added");
      } catch (err) {
        console.error("‚ùå Error adding ICE candidate:", err);
      }
    };

    const handleCallRequest = (data: {
      from: string;
      isVideo: boolean;
      callId: string;
    }) => {
      console.log("üìû ===== RECEIVED CALL_REQUEST =====");
      console.log(
        "üìû From:",
        data.from,
        "Video:",
        data.isVideo,
        "CallID:",
        data.callId
      );

      if (callStateRef.current !== "idle") {
        console.log("‚ö†Ô∏è Already in call, ignoring");
        return;
      }

      otherUserId.current = data.from;
      currentCallIdRef.current = data.callId;

      setIncomingCall({
        from: data.from,
        isVideo: data.isVideo,
        callId: data.callId,
      });
      setCallState("ringing");
      setIsVideoCall(data.isVideo);

      initializePeerConnection();

      callTimeoutRef.current = setTimeout(() => {
        console.log("‚è∞ Auto-declining call");
        declineCall();
        toast.error("Missed call");
      }, 45000);
    };

    const handleCallAccept = (data: { from: string; callId: string }) => {
      console.log("‚úÖ RECEIVED CALL_ACCEPT from:", data.from);

      if (callTimeoutRef.current) {
        clearTimeout(callTimeoutRef.current);
        callTimeoutRef.current = null;
      }

      setCallState("connecting");
    };

    const handleCallEnd = (data: { from: string }) => {
      console.log("üîö RECEIVED CALL_END from:", data.from);
      cleanup();
      toast.success("Call ended");
    };

    const handleCallDecline = (data: { from: string }) => {
      console.log("‚ùå RECEIVED CALL_DECLINE from:", data.from);
      cleanup();
      toast.error("Call declined");
    };

    const handleCallTimeout = (data: { callId: string }) => {
      console.log("‚è∞ RECEIVED CALL_TIMEOUT");
      setCallError("No answer");
      toast.error("Call timeout");
      setTimeout(() => cleanup(), 2000);
    };

    socket.on("offer", handleOffer);
    socket.on("answer", handleAnswer);
    socket.on("ice-candidate", handleIceCandidate);
    socket.on("call_request", handleCallRequest);
    socket.on("call_accept", handleCallAccept);
    socket.on("call_end", handleCallEnd);
    socket.on("call_decline", handleCallDecline);
    socket.on("call_error", handleCallError);
    socket.on("call_timeout", handleCallTimeout);
    socket.on('call_waiting', handleCallWaiting);

    return () => {
      socket.off("offer", handleOffer);
      socket.off("answer", handleAnswer);
      socket.off("ice-candidate", handleIceCandidate);
      socket.off("call_request", handleCallRequest);
      socket.off("call_accept", handleCallAccept);
      socket.off("call_end", handleCallEnd);
      socket.off("call_decline", handleCallDecline);
      socket.off("call_error", handleCallError);
      socket.off("call_timeout", handleCallTimeout);
      socket.off('call_waiting', handleCallWaiting);
    };
  }, [
    socket,
    isConnected,
    initializePeerConnection,
    declineCall,
    cleanup,
    localStream,
  ]);

  useEffect(() => {
    return () => {
      cleanup();
    };
  }, []);

  return {
    callState,
    connectionState,
    isVideoCall,
    callError,
    callDuration,
    isAudioMuted,
    isVideoMuted,
    isRemoteAudioMuted,
    isCallMinimized,
    localVideoRef,
    remoteVideoRef,
    localStream,
    remoteStream,
    peerConnectionRef,
    incomingCall,
    startCall,
    acceptCall,
    declineCall,
    endCall,
    toggleAudioMute,
    toggleVideoMute,
    toggleRemoteAudio,
    switchCallType,
    setIsCallMinimized,
    formatDuration,
  };
};